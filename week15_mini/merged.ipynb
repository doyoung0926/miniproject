{"cells":[{"cell_type":"markdown","source":["# [ **GitHub**에서 확인하시면 출력까지 확인하실 수 있습니다. ]\n","https://github.com/knudatascientists/Team1/blob/main/__%EC%B5%9C%EC%A2%85%EB%AA%A8%EB%8D%B8__/merged.ipynb\n","\n","---"],"metadata":{"id":"Bfx-YNmCX8Gl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2zD07_GXp9p"},"outputs":[],"source":["# Colab에서 구동시 True로 설정하고 실행\n","\n","Colab = False"]},{"cell_type":"markdown","metadata":{"id":"cqsvLj5iTzBj"},"source":["Colab용 구글드라이브 마운트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWybp2xpTx3q"},"outputs":[],"source":["if Colab:\n","    from google.colab import drive\n","\n","    drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSBVJQhoTx1K"},"outputs":[],"source":["# cd '/content/gdrive/MyDrive/Project_Methodology'"]},{"cell_type":"markdown","metadata":{"id":"pgTF_-vUTxD_"},"source":["## 모듈 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1ncmSaFTxEB"},"outputs":[],"source":["# if Colab:\n","#     !pip install beautifulsoup4\n","#     !pip install sentence_transformers\n","#     !pip install transformers\n","#     !pip install konlpy\n","#     !pip install selenium\n","#     !apt-get update\n","#     !apt install chromium-chromedriver\n","#     !cp /usr/lib/chromium-browser/chromedriver /usr/bin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SxBkcMRXacM"},"outputs":[],"source":["if Colab:\n","    from selenium import webdriver\n","    import time\n","    from selenium.webdriver.common.by import By\n","\n","    options = webdriver.ChromeOptions()\n","    options.add_argument('--headless') # browser를 띄우지 않고 실행하기\n","    options.add_argument('--no-sandbox') # sandbox 기능을 비활성화 하기\n","    options.add_argument('--disable-dev-shm-usage') # dev/shm/ 폴더를 사용하지 않기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_mCCoEQTxEC","outputId":"f409f5fa-29d3-46c0-ac7a-7d6a91e11d4d"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Miniconda3\\envs\\EV_PY39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# 모듈 불러오기\n","import numpy as np\n","import pandas as pd\n","import os\n","import re\n","from tqdm import tqdm\n","\n","# sklearn\n","from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, classification_report\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","\n","from urllib.request import urlopen #url 주소 호출 라이브러리\n","from bs4 import BeautifulSoup\n","from selenium import webdriver\n","from selenium.webdriver.common.keys import Keys\n","import time\n","\n","# tensorflow\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","from tensorflow.keras.utils import plot_model, to_categorical\n","from tensorflow.keras.optimizers import Adam\n","from keras.utils import np_utils\n","\n","# nlp\n","from konlpy.tag import Kkma, Komoran, Okt\n","from nltk.corpus import stopwords\n","import nltk as nlp\n","\n","from keras.models import load_model\n","\n","import itertools\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sentence_transformers import SentenceTransformer\n","from transformers import BertTokenizer, BertModel, TFBertModel\n","\n","\n","\n","import warnings \n","warnings.filterwarnings(action='ignore')"]},{"cell_type":"markdown","metadata":{"id":"VG9wH0f5TxED"},"source":["---  \n","---  \n","---"]},{"cell_type":"markdown","metadata":{"id":"_b6WMPAfTxED"},"source":["## 주제 분류 모델 첫번째"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtQj71FZTxED"},"outputs":[],"source":["data = pd.read_csv('./data/data.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5CHiK-dTxED"},"outputs":[],"source":["def makeTextlist(data):\n","    stopwords_01 = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n","    okt = Okt()\n","    text_list = []\n","    for text in tqdm(data['title']):\n","        text = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \" \", text) # 특수문자 제거\n","        text = text.strip() # 문자 처음과 끝 공백 제거\n","        tokens = okt.morphs(text) # 단어 추출\n","        text = [word for word in text if not word in stopwords_01] # 불용어 처리\n","        text = \"\".join(text)\n","        text = text.replace('  ',' ')\n","        text_list.append(text) \n","    \n","    data[\"title\"] = text_list "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jlesnfUjTxED","outputId":"14eb8c1e-e1f9-4c6a-f4f5-c49984e4128c"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 63931/63931 [01:09<00:00, 917.99it/s] \n"]}],"source":["makeTextlist(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qV8lXZjjTxED","outputId":"7879c3c7-eabb-4ece-e616-3f48bb796b38"},"outputs":[{"data":{"text/plain":["27"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# 단어 개수 column 추가\n","data['word_counts'] = [len(i.split(' ')) for i in data[\"title\"]]\n","data['word_counts'].max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWKjnElUTxEE"},"outputs":[],"source":["sent_length = data['word_counts'].max()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"GYnS8YipTxEE","outputId":"e7e1df6d-828d-4a49-9e2e-f148c5dfc61d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>topic_idx</th>\n","      <th>word_counts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>617</th>\n","      <td>지카바러스 규명 초저온전현미경 신약연구 유용</td>\n","      <td>0</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>622</th>\n","      <td>증강현실  알파고 어 포켓몬 고 거센 IT 광풍</td>\n","      <td>0</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>643</th>\n","      <td>AI 월드컵 생생 현장 중계</td>\n","      <td>0</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>654</th>\n","      <td>세돌 알파고 집중력 사람 기긴 어렵다 일문일답종합2보</td>\n","      <td>0</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>660</th>\n","      <td>올해 휴대폰 국내 생산량 2천500만대 10년전 18 4</td>\n","      <td>0</td>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                               title  topic_idx  word_counts\n","617         지카바러스 규명 초저온전현미경 신약연구 유용          0            5\n","622       증강현실  알파고 어 포켓몬 고 거센 IT 광풍          0            9\n","643                  AI 월드컵 생생 현장 중계          0            5\n","654    세돌 알파고 집중력 사람 기긴 어렵다 일문일답종합2보          0            7\n","660  올해 휴대폰 국내 생산량 2천500만대 10년전 18 4          0            8"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Under Sampling\n","minCounts = data['topic_idx'].value_counts(dropna=False).min()\n","\n","data0 = data[data[\"topic_idx\"] == 0][:minCounts]\n","data1 = data[data[\"topic_idx\"]== 1][:minCounts]\n","data2 = data[data[\"topic_idx\"]== 2][:minCounts]\n","data3 = data[data[\"topic_idx\"]== 3][:minCounts]\n","data4 = data[data[\"topic_idx\"]== 4][:minCounts]\n","data5 = data[data[\"topic_idx\"]== 5][:minCounts]\n","data6 = data[data[\"topic_idx\"]== 6][:minCounts]\n","data = pd.concat([data0, data1, data2, data3, data4, data5, data6], axis = 0)\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7wmoTPXTxEE","outputId":"c79fea9e-117a-4521-aaf6-b48edccfded8"},"outputs":[{"data":{"text/plain":["617           지카바러스 규명 초저온전현미경 신약연구 유용\n","622         증강현실  알파고 어 포켓몬 고 거센 IT 광풍\n","643                    AI 월드컵 생생 현장 중계\n","654      세돌 알파고 집중력 사람 기긴 어렵다 일문일답종합2보\n","660    올해 휴대폰 국내 생산량 2천500만대 10년전 18 4\n","Name: title, dtype: object"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["finalData = data['title']\n","finalData.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKjGcoHqTxEE"},"outputs":[],"source":["# target data 생성\n","target = data['topic_idx']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3dElMUo0TxEE"},"outputs":[],"source":["corpus = []\n","for i in finalData:\n","    corpus.append(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XoGaDB4_TxEF"},"outputs":[],"source":["tokenizer = Tokenizer(num_words=10000)  \n","tokenizer.fit_on_texts(corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hvLI6k65TxEF","outputId":"1e7c7247-7cc1-4edf-b22a-ecaf4e9392fb"},"outputs":[{"data":{"text/plain":["[8923, 1274, 5347]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# 정수 인코딩\n","encoded_docs = tokenizer.texts_to_sequences(corpus)\n","encoded_docs[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2HxZGwuTxEF","outputId":"2cfd531d-570a-4549-8717-902ff9af5fcd"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[   0    0    0 ... 8923 1274 5347]\n"," [   0    0    0 ...  451 3587 1297]\n"," [   0    0    0 ... 5960  567 2384]\n"," ...\n"," [   0    0    0 ... 8769  262 1475]\n"," [   0    0    0 ...  326 1555 1287]\n"," [   0    0    0 ... 4475  104 2853]]\n"]}],"source":["# 패딩\n","embedded_docs=pad_sequences(encoded_docs,padding='pre',maxlen=sent_length)\n","print(embedded_docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bCpLL-pTxEF"},"outputs":[],"source":["# 양방향 LSTM\n","def create_model():\n","\n","    model1=Sequential()\n","    model1.add(Embedding(10000,64,input_length=sent_length))\n","    model1.add(Bidirectional(LSTM(50)))\n","    model1.add(Dropout(0.3))\n","    model1.add(Dense(7,activation='softmax'))\n","    model1.compile(loss='CategoricalCrossentropy',optimizer='adam',metrics=['accuracy'])\n","    print(model1.summary())\n","    return model1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZoMhypE7TxEF","outputId":"84447947-f133-4498-80c6-7c4ced592d01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 27, 64)            640000    \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 100)              46000     \n"," l)                                                              \n","                                                                 \n"," dropout (Dropout)           (None, 100)               0         \n","                                                                 \n"," dense (Dense)               (None, 7)                 707       \n","                                                                 \n","=================================================================\n","Total params: 686,707\n","Trainable params: 686,707\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["model1 = create_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANdrNh2xTxEF"},"outputs":[],"source":["# numpy 배열로 변경\n","X_final=np.array(embedded_docs)\n","y_final=np.array(target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hs1px5SpTxEF"},"outputs":[],"source":["# ont-hot 인코딩\n","y_final = np_utils.to_categorical(y_final)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"harWxR6MTxEF"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdcfcVZvTxEF"},"outputs":[],"source":["X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZAPgptgTxEG"},"outputs":[],"source":["es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n","                   verbose=1, mode='min', baseline=None, restore_best_weights=True)\n","cp = ModelCheckpoint(\"./bidirectional_model.h5\" ,save_best_only = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeOlylkYTxEG","outputId":"d6fc40d3-ac4f-4db8-8515-429a454f1c26"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","303/303 [==============================] - 15s 38ms/step - loss: 1.2660 - accuracy: 0.5402 - val_loss: 0.7549 - val_accuracy: 0.7455\n","Epoch 2/10\n","303/303 [==============================] - 13s 44ms/step - loss: 0.5851 - accuracy: 0.8093 - val_loss: 0.6831 - val_accuracy: 0.7737\n","Epoch 3/10\n","303/303 [==============================] - 14s 45ms/step - loss: 0.4172 - accuracy: 0.8652 - val_loss: 0.7360 - val_accuracy: 0.7678\n","Epoch 4/10\n","303/303 [==============================] - 14s 47ms/step - loss: 0.3236 - accuracy: 0.8967 - val_loss: 0.8158 - val_accuracy: 0.7536\n","Epoch 5/10\n","302/303 [============================>.] - ETA: 0s - loss: 0.2650 - accuracy: 0.9145Restoring model weights from the end of the best epoch: 2.\n","303/303 [==============================] - 18s 61ms/step - loss: 0.2652 - accuracy: 0.9145 - val_loss: 0.8712 - val_accuracy: 0.7512\n","Epoch 5: early stopping\n"]}],"source":["hist = model1.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=10,callbacks=[es,cp],batch_size=100)"]},{"cell_type":"markdown","metadata":{"id":"zye7E5yYTxEG"},"source":["---  \n","---  \n","---  "]},{"cell_type":"markdown","metadata":{"id":"tAo9bQG_TxEG"},"source":["## 주제 분류 모델 두번째"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMzQNwE-TxEG"},"outputs":[],"source":["PATH = './data/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qjnTA37TxEG"},"outputs":[],"source":["train_data = pd.read_csv(PATH + \"data.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdG-1MriTxEG"},"outputs":[],"source":["test_data = train_data[51144:]\n","train_data = train_data[:51144]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxxssWcFTxEG"},"outputs":[],"source":["tf.random.set_seed(42)\n","np.random.seed(42)\n","\n","BATCH_SIZE = 32\n","NUM_EPOCHS = 1          # EPOCH 수 조정 --------------------------------------------------------------------------------------------------------------------------------------------------------------\n","VALID_SPLIT = 0.2\n","MAX_LEN = 44 \n","DATA_IN_PATH = 'data_in/KOR'\n","DATA_OUT_PATH = \"data_out/KOR\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6bYB5VITxEG"},"outputs":[],"source":["tokenizer_bert= BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt', do_lower_case=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K8cNBu9rTxEH"},"outputs":[],"source":["def bert_tokenizer(stc, MAX_LEN):\n","    \n","    encoded_dict = tokenizer_bert.encode_plus(\n","        text = stc,\n","        add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n","        max_length = MAX_LEN,           # Pad & truncate all sentences.\n","        pad_to_max_length = True,\n","        return_attention_mask = True    # Construct attn. masks.\n","        \n","    )\n","    \n","    input_id = encoded_dict['input_ids']\n","    attention_mask = encoded_dict['attention_mask'] # And its attention mask (simply differentiates padding from non-padding).\n","    token_type_id = encoded_dict['token_type_ids']  # differentiate two sentences\n","    \n","    return input_id, attention_mask, token_type_id"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azzk2XlgTxEH","outputId":"7c1234ee-d563-4587-e708-b83fe54cb43b"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/51144 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","100%|██████████| 51144/51144 [00:21<00:00, 2350.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["# sents: 51144, # labels: 51144\n"]}],"source":["input_ids = []\n","attention_masks = []\n","token_type_ids = []\n","train_data_labels = []\n","\n","for train_sent, train_label in tqdm(zip(train_data[\"title\"], train_data[\"topic_idx\"]), total=len(train_data)):\n","    try:\n","        input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN)   # 토큰화 및 패딩\n","        \n","        input_ids.append(input_id)\n","        attention_masks.append(attention_mask)\n","        token_type_ids.append(token_type_id)\n","        train_data_labels.append(train_label)\n","\n","    except Exception as e:\n","        print(e)\n","        print(train_sent)\n","        pass\n","\n","train_input_ids = np.array(input_ids, dtype=int)\n","train_attention_masks = np.array(attention_masks, dtype=int)\n","train_type_ids = np.array(token_type_ids, dtype=int)\n","train_inputs = (train_input_ids, train_attention_masks, train_type_ids)\n","\n","train_data_labels = np.asarray(train_data_labels, dtype=np.int32) \n","\n","print(\"# sents: {}, # labels: {}\".format(len(train_input_ids), len(train_data_labels)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X3zEMHOjTxEH","outputId":"ed587af0-0fb3-4b81-dc05-d0137afdcb22"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}],"source":["# MODEL CLASS ------------------------------------------------------------------------------------------------------------------------------------\n","class TFBertClassifier(tf.keras.Model):\n","    def __init__(self, model_name, dir_path, num_class):\n","        super(TFBertClassifier, self).__init__()\n","\n","        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n","        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n","        self.classifier = tf.keras.layers.Dense(num_class, \n","                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n","                                                name=\"classifier\")\n","        \n","    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n","        \n","        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n","        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        pooled_output = outputs[1] \n","        pooled_output = self.dropout(pooled_output, training=training)\n","        logits = self.classifier(pooled_output)\n","\n","        return logits\n","\n","# ---------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","model2 = TFBertClassifier(model_name='bert-base-multilingual-cased',\n","                                  dir_path='bert_ckpt',\n","                                  num_class=7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7M4fQ4QjTxEH"},"outputs":[],"source":["optimizer = tf.keras.optimizers.Adam(3e-5)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","model2.compile(optimizer=optimizer, loss=loss, metrics=[metric])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyRbpREtTxEH","outputId":"ff7950fe-77a6-48de-d71c-ec3216989d11"},"outputs":[{"name":"stdout","output_type":"stream","text":["data_out/KOR\\BERT_klue/bert-base -- Folder already exists \n","\n"]}],"source":["model_name = \"BERT_klue/bert-base\"\n","\n","\n","earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n","\n","checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","\n","# Create path if exists\n","if os.path.exists(checkpoint_dir):\n","    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n","else:\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n","    \n","cp_callback = ModelCheckpoint(\n","    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wkp-VpftTxEI","outputId":"e8f7195d-ae3c-4037-e29f-e0d58c5bf027"},"outputs":[{"name":"stdout","output_type":"stream","text":["1279/1279 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.8100\n","Epoch 1: val_accuracy improved from -inf to 0.55900, saving model to data_out/KOR\\BERT_klue/bert-base\\weights.h5\n","1279/1279 [==============================] - 8485s 7s/step - loss: 0.5617 - accuracy: 0.8100 - val_loss: 1.6473 - val_accuracy: 0.5590\n"]}],"source":["history = model2.fit(train_inputs, train_data_labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n","                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"]},{"cell_type":"markdown","metadata":{"id":"pDhg_9GgTxEI"},"source":["---  \n","---  \n","---  "]},{"cell_type":"markdown","metadata":{"id":"mdtn0xSgTxEI"},"source":["#### 첫번째 모델로 타이틀 분류해주는 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRVPlLS7TxEI"},"outputs":[],"source":["def getSubject01(list_title_result) : \n","    # 각각의 기사 제목 마다 분류된 값을 저장할 리스트\n","    list_Subject01 = []\n","    # 각각의 기사 제목 리스트에서\n","    for i in range(0, len(list_title_result)):\n","        # 토큰화 해주고\n","        test = tokenizer.texts_to_sequences([list_title_result[i]])\n","        # 패딩해주고\n","        text = pad_sequences(test, padding='pre', maxlen = 27)\n","        # 예측값을 저장\n","        list_Subject01.append(model1.predict(text).argmax())\n","    # 저장된 리스트를 반환\n","    return list_Subject01"]},{"cell_type":"markdown","metadata":{"id":"0obYoMt1TxEI"},"source":["#### 두번째 모델로 타이틀 분류해주는 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pa8CSHoLTxEI"},"outputs":[],"source":["def getSubject02(list_title_result) : \n","    # 각각의 기사 제목 마다 분류된 값을 저장할 리스트\n","    list_Subject02 = []\n","\n","    input_ids = []\n","    attention_masks = []\n","    token_type_ids = []\n","    data_labels = []\n","\n","    # 토큰화 및 패딩\n","    for sent in tqdm(data[\"title\"]): \n","        try:\n","            input_id, attention_mask, token_type_id = bert_tokenizer(sent, MAX_LEN)\n","\n","            input_ids.append(input_id)\n","            attention_masks.append(attention_mask)\n","            token_type_ids.append(token_type_id)\n","        except Exception as e:\n","            print(e)\n","            print(sent)\n","            pass\n","\n","    input_ids = np.array(input_ids, dtype=int)\n","    attention_masks = np.array(attention_masks, dtype=int)\n","    type_ids = np.array(token_type_ids, dtype=int)\n","    inputs = (input_ids, attention_masks, type_ids)\n","\n","    # 결과값 예측\n","    results = model2.predict(inputs, batch_size=1024)\n","\n","    # 예측값을 저장\n","    for i in range(len(results)):\n","        list_Subject02.append(np.argmax(results[i]))\n","\n","    # 저장된 리스트를 반환\n","    return list_Subject02"]},{"cell_type":"markdown","metadata":{"id":"Hp3lA3uXTxEI"},"source":["#### 내용 요약해주는 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTbF11mJXp93"},"outputs":[],"source":["def getKeyword(list_text):\n","    # 각각의 기사 내용을 요약된 값들을 저장할 리스트\n","    list_Keyword = []\n","    # 형태소 분류기 호출\n","    okt = Okt()\n","    # 저장된 모델 호출\n","    model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n","\n","    # 각각의 기사 내용을\n","    for i in range(0, len(list_text)) :\n","        # 토큰화 하고\n","        tokenized_doc = okt.pos(list_text[i])\n","        # 명사들만 구성된 문장을 만든다\n","        tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n","\n","        # 단어 묶음 설정, 하나 혹은 2개의 단어로 구성된 요약 형성\n","        n_gram_range = (1, 2)\n","\n","        # countVectorizer\n","        count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n","\n","        candidates = count.get_feature_names_out()\n","        doc_embedding = model.encode([list_text[i]])\n","        candidate_embeddings = model.encode(candidates)\n","\n","        # 요약된 단어는 3개만 도출\n","        top_n = 3\n","        # 요약된 단어간 거리 조절\n","        distances = cosine_similarity(doc_embedding, candidate_embeddings)\n","        # 거리에 따른 요약된 단어를 3개만 도출\n","        keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n","        # 도출된 단어 리스트를 전체 리스트에 저장\n","        list_Keyword.append(keywords)\n","    # 전체 리스트 반환\n","    return list_Keyword"]},{"cell_type":"markdown","metadata":{"id":"WMK37SsZTxEJ"},"source":["#### 출력 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fveI3dX4TxEJ"},"outputs":[],"source":["def showResult(list_title, list_url, list_Subject01, list_Subject02, list_Keyword) :\n","    # 값들이 저장된 딕셔너리 형성\n","    dic = {0 : 'IT', 1 : '경제', 2 : '사회', 3 : '생활문화', 4 : '세계', 5 : '스포츠', 6 : '정치'}\n","    # 전체 기사에 대한\n","    for i in range(0, len(list_title)) :\n","        print('-----------------------------------------------------------')\n","        print(f'기사 제목 : {list_title[i]}')   # 제목\n","        print(f'상세 기사 : {list_url[i]}')     # url\n","        print(f'#{dic[list_Subject01[i]]}, #{dic[list_Subject02[i]]}, #{list_Keyword[i][0]}, #{list_Keyword[i][1]}, #{list_Keyword[i][2]}') #분류(1), #분류(2), #요약(1), #요약(2), #요약(3)\n","        print('-----------------------------------------------------------')"]},{"cell_type":"markdown","metadata":{"id":"uqCdUvRiTxEJ"},"source":["#### 뉴스 크롤링 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"K3MzccyTTxEJ"},"outputs":[],"source":["def getNews(n) :\n","    # 크롤링한 자료들을 저장할 리스트\n","    list_url_result = []    # url\n","    list_title_result = []  # 제목\n","    list_text_result = []   # 내용\n","\n","    # 드라이버 호출\n","    if Colab:                                  \n","        driver = webdriver.Chrome('chromedriver',options=options)\n","    else:\n","        driver = webdriver.Chrome(\"./chromedriver\")    \n","                          \n","    url = 'https://news.naver.com/' # url 주소 설정\n","    driver.get(url) # 해당 주소로 드라이버 실행\n","    driver.implicitly_wait(3)  # 크롤링 방지를 방지하기 위한 대기 3초...\n","    html = driver.page_source # html 불러오기\n","    soup = BeautifulSoup(html, 'html.parser')   # BeautifulSoup으로 정리\n","\n","    # 페이지에 있는 모든 href 받아오기\n","    list_url = [] # url 저장할 리스트\n","    list_a = soup.find_all('a') # a가 들어간 모든 부분에서\n","    for i in range(0, len(list_a)):\n","        if len(list_a[i]['href']) >= 1 : # href가 있으면 해당 부분 저장\n","            list_url.append(list_a[i]['href'])\n","\n","    driver.close()   # 현재 탭 닫기\n","    driver.quit()    # 브라우저 닫기\n","\n","    # 전체 list_url 에서 num 개의 뉴스 url만 따로 저장\n","    list_url_02 = []\n","    for j in range(91, (91 + (2 * n)), 2) : # n개만\n","        list_url_02.append(list_url[j])\n","\n","    # 따로 저장한 url 에서 해당 기사의 제목, url, 기사 내용 추출\n","    for k in range(0, len(list_url_02)) :\n","        url = list_url_02[k]\n","        html = urlopen(url) # url 주소 html로 저장\n","        soup = BeautifulSoup(html.read(), 'html.parser') # html 데이터 BeautifulSoup으로 요약\n","        a = soup.contents[2].text\n","        t = a.split('\\n')\n","\n","        # 기사 페이지에서 가장 긴 부분(= 기사 내용) 부분 인덱스 값 저장\n","        list_len = []\n","        for l in range(0, len(t)):\n","            list_len.append(len(t[l]))\n","        num = np.argmax(list_len)\n","\n","        # 타이틀 받아오기\n","        title = soup.title.text\n","\n","        list_url_result.append(url)     # url 저장\n","        list_title_result.append(title) # 제목 저장\n","        list_text_result.append(t[num]) # 내용 저장\n","\n","    ####\n","    print(list_title_result)\n","    ####\n","    \n","    list_Subject01 = getSubject01(list_title_result)    # getSubject01 함수로 기사별 분류 값들을 저장\n","    list_Subject02 = getSubject02(list_title_result)    # getSubject02 함수로 기사별 분류 값들을 저장\n","    list_Keyword = getKeyword(list_text_result)         # getKeyword 함수로 기사별 요약 값들을 저장\n","\n","    # 기사 제목, 기사 주소, 기사 분류(1), 기사 분류(2), 기사 요약 값들을 출력 함수에 전달\n","    showResult(list_title_result, list_url_result, list_Subject01, list_Subject02, list_Keyword)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"O4qs6fYBTxEJ","outputId":"ebc26b03-e007-4a22-f7d5-e7e838f8109c"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b16456cb7914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 지금 네이버 뉴스에 있는 10개의 기사 크롤링 후 분류 및 요약\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgetNews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-98730ba7a8cf>\u001b[0m in \u001b[0;36mgetNews\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# 드라이버 호출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mColab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chromedriver'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Colab' is not defined"]}],"source":["# 지금 네이버 뉴스에 있는 10개의 기사 크롤링 후 분류 및 요약\n","getNews(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exVJCMMgXp94"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["tAo9bQG_TxEG","mdtn0xSgTxEI","0obYoMt1TxEI","Hp3lA3uXTxEI","WMK37SsZTxEJ","uqCdUvRiTxEJ"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.13 ('EV_PY39')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"c34f60c4b6508d34c0193a1e5ad16bfc3f81e2e087a9e2f73ef284bcb9806a0b"}}},"nbformat":4,"nbformat_minor":0}